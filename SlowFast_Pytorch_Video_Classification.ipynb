{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTWKJ8UiqC1XbPORlhhizn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zeeshann1/SlowFast-Network-Video-Classification/blob/main/SlowFast_Pytorch_Video_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LuYz8uCWGYgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0ddafe-28ba-44ef-ecfa-649dd8c565ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/pytorchvideo.git\n",
            "  Cloning https://github.com/facebookresearch/pytorchvideo.git to /tmp/pip-req-build-_t5bu3_b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorchvideo.git /tmp/pip-req-build-_t5bu3_b\n",
            "  Resolved https://github.com/facebookresearch/pytorchvideo.git to commit 1730313bb5eda307657f971365de7a1359e1a68e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av\n",
            "  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parameterized\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo==0.1.5) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo==0.1.5) (1.22.4)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo==0.1.5) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo==0.1.5) (4.65.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo==0.1.5) (2.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo==0.1.5) (8.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo==0.1.5) (0.8.10)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo==0.1.5) (4.5.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=211228 sha256=7047e0a3bb96aa75a13b786029319d4442c186b665f6eddb7fc2846195c1a436\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-01ydeho_/wheels/09/02/d5/6f0c4d48bfb2f965685a49f16dd79b7c56f9774ac283c76157\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61429 sha256=cbfe62df2917f2a9e5b795bf6fb2fbae0b8cd689edf10da55632e045958da57f\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=710acef2d5ea3b394302ce36d306279d5263e765ef019820a80c52397a3118f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built pytorchvideo fvcore iopath\n",
            "Installing collected packages: av, yacs, portalocker, parameterized, iopath, fvcore, pytorchvideo\n",
            "Successfully installed av-10.0.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-2.7.0 pytorchvideo-0.1.5 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import torch\n",
        "except ModuleNotFoundError:\n",
        "    !pip install torch torchvision\n",
        "    import os\n",
        "    import sys\n",
        "    import torch\n",
        "    \n",
        "if torch.__version__=='1.6.0+cu101' and sys.platform.startswith('linux'):\n",
        "    !pip install pytorchvideo\n",
        "else:\n",
        "    need_pytorchvideo=False\n",
        "    try:\n",
        "        # Running notebook locally\n",
        "        import pytorchvideo\n",
        "    except ModuleNotFoundError:\n",
        "        need_pytorchvideo=True\n",
        "    if need_pytorchvideo:\n",
        "        # Install from GitHub\n",
        "        !pip install \"git+https://github.com/facebookresearch/pytorchvideo.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ") \n",
        "from typing import Dict"
      ],
      "metadata": {
        "id": "arR4M_i_G8M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc66bc27-e71b-4e64-a8c9-4acaf1568c7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XFkJJ24G8l6",
        "outputId": "57e3588d-7442-4f90-ffcb-d532ec91ab36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-08 07:58:43--  https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 52.84.162.119, 52.84.162.51, 52.84.162.103, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|52.84.162.119|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10326 (10K) [text/plain]\n",
            "Saving to: ‘kinetics_classnames.json’\n",
            "\n",
            "\rkinetics_classnames   0%[                    ]       0  --.-KB/s               \rkinetics_classnames 100%[===================>]  10.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-08 07:58:43 (257 MB/s) - ‘kinetics_classnames.json’ saved [10326/10326]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
        "    kinetics_classnames = json.load(f)\n",
        "\n",
        "# Create an id to label name mapping\n",
        "kinetics_id_to_classname = {}\n",
        "for k, v in kinetics_classnames.items():\n",
        "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
      ],
      "metadata": {
        "id": "mH48tilBG8pL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device on which to run the model\n",
        "# Set to cuda to load on GPU\n",
        "device = \"cpu\"\n",
        "\n",
        "# Pick a pretrained model \n",
        "model_name = \"slowfast_r50\"\n",
        "model = torch.hub.load(\"facebookresearch/pytorchvideo:main\", model=model_name, pretrained=True)\n",
        "\n",
        "# Set to eval mode and move to desired device\n",
        "model = model.to(device)\n",
        "model = model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sexl8EkBLYfM",
        "outputId": "370de3d4-551c-4513-b7ee-3326c125704e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/pytorchvideo/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_8x8_R50.pyth\" to /root/.cache/torch/hub/checkpoints/SLOWFAST_8x8_R50.pyth\n",
            "100%|██████████| 264M/264M [00:01<00:00, 201MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# SlowFast transform\n",
        "####################\n",
        "\n",
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "alpha = 4\n",
        "\n",
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors. \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second"
      ],
      "metadata": {
        "id": "08yejEHZLclc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the example video file\n",
        "!wget https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJULltI1Lc8e",
        "outputId": "19c83f2b-18d8-4eed-c9de-60e09ddb0dfe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-08 07:58:46--  https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 52.84.162.119, 52.84.162.51, 52.84.162.103, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|52.84.162.119|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 549197 (536K) [video/mp4]\n",
            "Saving to: ‘archery.mp4’\n",
            "\n",
            "archery.mp4         100%[===================>] 536.33K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-05-08 07:58:46 (12.5 MB/s) - ‘archery.mp4’ saved [549197/549197]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the example video\n",
        "video_path = \"archery.mp4\"  \n",
        "\n",
        "# Select the duration of the clip to load by specifying the start and end duration\n",
        "# The start_sec should correspond to where the action occurs in the video\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration \n",
        "\n",
        "# Initialize an EncodedVideo helper class\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "\n",
        "# Load the desired clip\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "# Apply a transform to normalize the video input\n",
        "video_data = transform(video_data)\n",
        "\n",
        "# Move the inputs to the desired device\n",
        "inputs = video_data[\"video\"]\n",
        "inputs = [i.to(device)[None, ...] for i in inputs]"
      ],
      "metadata": {
        "id": "eWfh8nR-ZMW7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the input clip through the model \n",
        "preds = model(inputs)"
      ],
      "metadata": {
        "id": "tCMtw0qRZMyt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes \n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes[0]]\n",
        "print(\"Predicted labels: %s\" % \", \".join(pred_class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9W6u17pZWJC",
        "outputId": "ad9f6368-eb04-4155-ce9a-166eb4ee4d66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"
          ]
        }
      ]
    }
  ]
}